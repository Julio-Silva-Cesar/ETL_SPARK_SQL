# **Pipeline de Controle de Estoque Comercial**
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![BigQuery](https://img.shields.io/badge/BigQuery-4285F4?style=for-the-badge&logo=googlecloud&logoColor=white)


Pipeline ETL para controle de equipamentos alocados a colaboradores do setor comercial com integra√ß√£o de dados de RH.

## üìã Vis√£o Geral
O script √© estruturado seguindo as etapas:
* Instala√ß√£o de Deped√™ncias
* Integra√ß√£o com banco com o google Drive e captura de dados do DE/PARA de itens contido em planilha google sheets
* Encaminhamento para tabela DeltaLake Databricks
* Processamento em e SPARK/SQL para ETL dos dados
* Classifica√ß√£o dos itens do estoque com base em suas respectivas classes
* Integra√ß√£o com dados de RH
* Integra√ß√£o com o BigQuery para atualiza√ß√£o do banco de dados
* Alimenta dashboard em Looker Studio 



## üîÑ Fluxo do Processo
```mermaid
flowchart TD
    A[Google Sheets] -->|Captura De/Para| B[Databricks]
    B -->|Delta Lake| C[Cat√°logo de Itens]
    C -->|Spark SQL| D[Processamento ETL]
    D -->|Transforma√ß√£o| E[Classifica√ß√£o de Itens]
    E -->|Pandas| F[BigQuery]
    F -->|Visualiza√ß√£o| G[Looker Studio]
```

## ‚öôÔ∏è Pr√©-requisitos
- Python 3.8+
- Databricks Runtime
- Conta de servi√ßo Google Cloud
- Acesso ao BigQuery

## üìÅ Estrutura do c√≥digo

## üîß Instala√ß√£o

```bash
# No notebook Databricks:
%pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client gspread drive pandas_gbq gspread_dataframe
unidecode numpy datetime packaging==23.2 -q

dbutils.library.restartPython()

```
**Finalidade das bibliotecas**
* Realiza as Autentica√ß√µes Google/bigquery (OAuth2)
* Manipula√ß√£o de planilhas (gspread)
* Integra√ß√£o BigQuery (pandas_gbq)
* Processamento de dados (Pandas/NumPy)
* Reinicializa√ß√£o do ambiente Python
  
## üåê Integra√ß√£o com Google Sheets

```bash

SCOPES = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.comauth/drive"]
SERVICE_ACCOUNT_FILE = '/Workspace/Users/.../gsa_dados_drive.json'
credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
client = gspread.authorize(credentials)

```
* Os SCOPES definem as permiss√µes de leitura/escrita que ser√£o utilizadas para acessar o ambiente google
* Autentica√ß√£o via conta de servi√ßo do google drive para manioula√ß√£o das planilhas
* Cria√ß√£o do cliente via objeto de credencial gerado pelo uso de arquivo JSON da conta de servi√ßo.
  
**Ap√≥s a integra√ß√£o o c√≥digo abaixo:**

* Acessa a planilha pelo ID definido no par√¢metro open_by_key
* Seleciona a aba espec√≠fica
* Converte os dados para DataFrame Pandas

```bash
spreadsheet = client.open_by_key('ID_PLANILHA')
worksheet = spreadsheet.worksheet("nome_aba_planilha")
ss = pd.DataFrame(worksheet.get_all_records())
```
**Encaminhamento da tabela para o Delta Lake Catalog/Databricks via spark**
```bash
spark.createDataFrame(ss).write.format("delta")
    .mode("overwrite")
    .saveAsTable("database.schema.depara_itens")
```
* Cria tabela Delta Lake versionada
* Estrutura: database.schema.table_name
* Modo overwrite para atualiza√ß√µes completas
  
## üßë‚Äçüíª ETL principal: processamento em SPARK/SQL (EM DEV)

